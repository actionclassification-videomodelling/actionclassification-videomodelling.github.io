<!DOCTYPE html>
<!--[if lt IE 7]>      <html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]>         <html class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]>         <html class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js"> <!--<![endif]-->
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <title>Brave new ideas in motion representations</title>
    <meta name="description" content="">
    <meta name="viewport" content="width=device-width">

    <link rel="stylesheet" href="css/bootstrap.min.css">
    <link rel="stylesheet" href="css/bootstrap-responsive.min.css">
    <link rel="stylesheet" href="css/font-awesome.min.css">
    <link rel="stylesheet" href="css/main.css">
    <link rel="stylesheet" href="css/sl-slide.css">

    <script src="js/vendor/modernizr-2.6.2-respond-1.1.0.min.js"></script>
    <script src="data/id2path.json" charset="utf-8"></script>
    

    <script src="js/animate.js"></script>

    <!-- Le fav and touch icons -->
    <link rel="shortcut icon" href="images/ico/favicon.ico">
    <link rel="apple-touch-icon-precomposed" sizes="144x144" href="images/ico/apple-touch-icon-144-precomposed.png">
    <link rel="apple-touch-icon-precomposed" sizes="114x114" href="images/ico/apple-touch-icon-114-precomposed.png">
    <link rel="apple-touch-icon-precomposed" sizes="72x72" href="images/ico/apple-touch-icon-72-precomposed.png">
    <link rel="apple-touch-icon-precomposed" href="images/ico/apple-touch-icon-57-precomposed.png">
</head>

<body>

<!--     Header-->
    <header class="navbar navbar-fixed-top">
        <div class="navbar-inner">
            <div class="container">
                <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </a>
                <a id="logo" class="pull-left" href="index.html"></a>
                <div class="nav-collapse collapse pull-right">
                    <ul class="nav">
                        <li><a href="index.html">Home</a></li>
                        <li><a href="#about">About</a></li>
                        <li><a href="#program">Program</a></li>
                        <li><a href="#dates">Dates</a></li>
                        <li><a href="#submission">Submission</a></li> 
                        <li><a href="#registration">Registration & Venue</a></li>
                        <li><a href="#organizers">Organizers</a></li>
                    </ul>        
                </div><!--/.nav-collapse -->
            </div>
        </div>
    </header>
    <!-- /header -->
       
</section>
<!--/Slider-->

<section id="about" >
    <div class="container">
        <div class="row-fluid">
            <div class="span9">
                <h1>CVPR 2019 Tutorial on <br/>Action Classification and Video Modelling</h1>
                

                <p class="lead">Together with the <a href="http://cvpr2019.thecvf.com/">Computer Vision and Pattern Recognition (CVPR)</a> 2019.
                </p> 
                

                <h3>Description of the tutorial and its relevance</h3>
                <p class="lead">
                In the late years Deep Learning has been a great force of change on most Computer Vision and Machine Learning tasks. 
                In video analysis problems, such as action recognition and detection, motion analysis and tracking, the progress has arguably been slower, with shallow models remaining surprisingly competitive.
                Recent developments (Tran et al., 2014), (Feichtenhofer et al., 2016), (Carreira et al., 2017), (Tran et al., 2015), (Bilen et al. 2017), (Ghodrati et al., 2018), (Feichtenhofer et al. 2018), (Simonyan et al., 2014), (Kalchbrenner et al., 2017) have demonstrated that careful model design and end-to-end model training, as well as large and well-annotated datasets, have finally led to strong results using deep architectures for video analysis. 
                However, the details and the secrets to achieve good accuracies with deep models are not always transparent. 
                Furthermore, it is not always clear whether the networks resulting from end-to-end training are truly providing better video models or if instead are simply overfitting their large capacity to the idiosyncrasies of each dataset.
                </p>

                <p>This tutorial aims at giving answers to the aforementioned questions.
                  Specifically, the core topics to be explored are</p>

                <p class="lead">
                - What are the state-of-the-art video representations and action classification models and how does one train them?<br/>
                - What does constitute a strong video representation?<br/>
                - Are short and long videos to be treated equally when training action classifiers?<br/>
                - Are shallow models still relevant for state-of-the-art classification? <br/>
                - How to train an action classification system in an unsupervised manner, when supervised labels are not enough?<br/>
                - Relevant benchmarks and challenges<br/>
                </p>

                <p>This tutorial is organized by experts on action classification and video representation learning: a) Dr. E. Gavves, Assistant Professor in the University of Amsterdam, The Netherlands, b) J. Carreira, Research Scientist in DeepMind, UK, c) Dr. B. Fernando, Research Scientist A*Star, Singapore, d) Dr. C. Feichtenhofer, Senior Researcher in Facebook FAIR, US and, e) Dr. L. Torresani, Associate Professor at Dartmouth College and Research Scientist at Facebook FAIR, USA.
                  </p>

                <h3>Topics</h3>
                <p class="lead">
                The tutorial focuses on the following topics: </p>
               
               <p class="lead">
                - Deep learning for action classification and optical flow. Discuss the latest modern deep networks for action classification, including C3D (Tran et al., 2014), I3D (Carreira et al. 2017), Two-Stream models (Simonyan et al. 2014), Two-stream-fusion (Feichtenhofer et al., 2016), Dynamic Images(Bilen et al., 2016).<br/>
                - Deep networks for video modeling. Discuss and analyze various options for modeling videos, including TSN (Wang et al., 2018), spatiotemporal (Tran et al. 2015) and factorized spatiotemporal convolutions (Tran et al., 2018), TimeAligned DenseNets (Ghodrati et al., 2018), Dynamic Image Networks (Bilen et al. 2017).<br/>
                - Deep spatiotemporal models beyond classification. While in the Computer Vision community video models have been designed primarily for action classification, their applicability extends to video generative models (Kalchbrenner et al., 2017), video compression (Wu et al., 2018), visualization (Feichtenhofer et al., 2018), velocity estimation (Kampelmuhler et al., 2018), tracking (Tao et al., 2016) and spatiotemporal object detection (Feichtenhofer et al., 2017), and future video prediction (Ghodrati et al., 2018).<br/>
                - Unsupervised video representation learning. Analogously to the still-image domain, even for video it has been customary to fine-tune pretrained video models on the target dataset. However, this is not always optimal, due to large gaps between the source and the target domain~\cite{ (Shkodrani et al., 2018) or because of unconventional architectures (Ghodrati et al., 2018). More importantly, while supervised learning certainly results in highly accurate models, it does not take advantage of the plethora of unlabelled video available. We discuss alternatives on training video representations models either in an unsupervised or self-supervised manner, including arrow-of-time (Wei et al., 2018), audio-video synchronization, or odd-one-out models (Fernando et al., 2017).<br/>
                - Long-term video understanding. The majority of action classification and video representation systems focus on rather short video sequences, typically no more than 10 seconds long. However, applications often require processing much longer videos, or even streaming videos. We discuss models that are specifically designed to handle long videos and capture the spatiotemporal intricacies involved, like Timeception (Hussein et al, 2019) and VideoGraph (Hussein et al., 2019).<br/>
                - Large-scale video processing and evaluation. Careful evaluation of action classifiers and video representations is crucial for developing the next generation of models. Interestingly, while current benchmarks do measure well the accuracy of action classifiers, it is not always clear how to evaluate the capacity of temporal models in modeling the sequence itself. We discuss various benchmarks and frameworks for evaluating action classification, as well as for evaluating directly video representations.<br/>
                </p>

                
            </div>
        </div>
    </div>
</section>



<section id="services">   

<div class="container" id ="program">
        <div class="row-fluid">
            <div class="span9">
                <h1>Program</h1>
                <p class="lead">
                Date: 21 july 2017.
                </p>
		<table class="lead" style="border-style: groove;width:1200px" border="1" >
		<tr>
			<th width="20%">Time</th>
    			<th width="40%">Event</th>
    			<th width="40%">Description</th>
  		</tr>
		<tr><td>8.45 - 9.00   </td> <td> Welcome to the workshop </td> <td> Information</td> </tr>
                <tr><td>9.00 - 10.00   </td> <td> Invited speaker 1  </br> <a href="http://people.csail.mit.edu/mrub/"> Dr. Miki Rubinstein</a> </td> <td> Talk 1 </td> </tr>
		<tr><td>10.00 - 10.20  </td> <td> Break </td> <td> Coffee </td> </tr>
                <tr><td>10.20 - 11.20  </td> <td> Invited speaker 2 </br>   <a href="http://cis.jhu.edu/~rvidal/">Professor Rene Vidal</a> </td> <td>  Talk 2 </td> </tr>
                <tr><td>11.25 - 11.45  </td> <td> <a href="https://openreview.net/pdf?id=HJfDVHkag">Unsupervised Human Action Detection by Action Matching</a> </td> <td>Oral Presentation 1 by Basura Fernando, Sareh Shirazi, Stephen Gould </td> </tr>
		<tr><td>11.45 - 13.30  </td> <td> Lunch (on your own) </td> <td> Poster session </td> </tr>

                <tr><td>13.30 - 14.15  </td> <td> Invited speaker 4 </br>  <a href="http://www.cs.rochester.edu/u/jluo/">Professor Jiebo Luo</a>  </td> <td> Talk 4  </td> </tr>
                <tr><td>14.15 - 14.30  </td> <td> <a href="https://openreview.net/pdf?id=BJrN0GhTl">RATM: Recurrent Attentive Tracking Model</a> </td> <td> Oral Presentation 2 by Samira Ebrahimi Kahou, Vincent Michalski, Roland Memisevic, Christopher Pal, Pascal Vincent </td> </tr>
		<tr><td>14.30 - 14.45  </td> <td> Break </td> <td> Coffee </td> </tr>
                <tr><td>14.45 - 15.30  </td> <td> Invited speaker 5 </br> <a href="http://www.ceessnoek.info/">Professor Cees G.M. Snoek</a></td> <td> Talk 5 </td> </tr>

                <tr><td>15.30 - 15.45  </td> <td> <a href="https://openreview.net/pdf?id=BJWBzOBTx">Interpretable 3D Human Action Analysis with Temporal Convolutional Networks</a> </td> <td> Oral Presentation 3 by Tae Soo Kim, Austin Reiter </td> </tr>
                <tr><td>15.45 - 16.00  </td> <td> <a href="https://openreview.net/pdf?id=H1gaV4mae">Learning Dynamic GMM for Attention Distribution on Single-face Videos </a> </td> <td> Oral Presentation 4 by Yun Ren, Zulin Wang, Mai Xu, Haoyu Dong, Shengxi Li </td> </tr>
                <!--tr><td>16.00 - 16.15  </td> <td> <a href="https://openreview.net/pdf?id=SJcvXCBax">Optical Acceleration for Motion Description in Videos </a></td> <td> Oral Presentation 5 by Anitha Edison, Jiji C. V. </td> </tr-->
                <tr><td>16.00 - 16.45  </td> <td> Invited speaker 3 </br>  <a href="http://www.stat.ucla.edu/~yuille/">Professor Alan Yuille </a>  </td> <td>  Talk 3 </td> </tr>
		<tr><td>16.45 - 17.30  </td> <td> Poster session  </td> <td> Poster session  </td> </tr>		
		</table>
                
            </div>
        </div>
    </div>

</section>



<section id="dates">
    <div class="container" >
        <div class="center">
            <h3>Important Dates</h3>
            <p class="lead">Together with the <a href="http://cvpr2017.thecvf.com/">Computer Vision and Pattern Recognition (CVPR)</a> 2017.</p>
            <p class="lead">Date of the workshop: 21 July 2017 </p>
        </div>  
        <div class="gap"></div>
        <ul class="gallery col-4">
            <!--Item 1-->	     
	
	    <!--Item 1.1-->
	    <li>	
             <p class="lead">4 Page Submission Deadline</p>
                <div class="desc">
                    <h5 style="color:red;"><strike>15th May 2017 </strike></h5>
                </div>
            </li>
            <!--/Item 1.1--> 	 

            <!--Item 2-->
            <li>
                <p class="lead">4 Page Acceptance</p>
                <div class="desc">
                    <h5><strike>22nd May 2017</strike></h5>
                </div>
            </li>

            <li>
                <p class="lead">8 Page Acceptance</p>
                <div class="desc">
                    <h5><strike>3rd May 2017</strike></h5>
                </div>
            </li>	
            <!--/Item 2-->

            <!--Item 3-->
            <li>
                <p class="lead">8 Page Camera ready</p>
                <div class="desc">
                    <h5><strike>15th May 2017</strike></h5>
                </div>
            </li>
            <!--/Item 3--> 

                          

        </ul>
    </div>

</section>

<section id="services">
    <div class="container" id="submission">
        <div class="row-fluid">
            <div class="span9">

                <h1>Submission</h1>                
                <h3>Constructive discussion</h3>
                <p class="lead">
                The workshop's goal is a constructive, creative and open conversation. In principle we will accept all papers. All reviews will be made publicly available. Reviewers can choose to remain anonymous or to reveal their identity to encourage collaboration and positive feedback. We include poster presentations and will select a few of the best and bravest papers for an oral presentation. 
                </p>

		<h3>Instructions</h3>                
                <p class="lead">
		You can submit papers in two different formats. <br><br><strike>
1.Full paper submission should include 8 pages of text and should use the CVPR 2017 camera ready format as per the instructions given <a href="http://cvpr2017.thecvf.com/submission/main_conference/author_guidelines">here</a>. Full  paper submission should include 8 pages (excluding references) and will be included in the proceedings of the CVPR17 workshops. Therefore, the deadline for full paper submission is 7th April 2017.<br><br></strike>

2. Authors can also submit 4 Page papers which will be peer reviewed. However, they will not be include in the proceedings. Please follow the 
 CVPR 2017 camera ready format as per the instructions given <a href="http://cvpr2017.thecvf.com/submission/main_conference/author_guidelines">here</a> but limit your paper to 4 pages excluding references.
<br><br>

All papers should have the names of the authors, institute and the email address in the header of the paper as per the camera ready format of CVPR 2017. Authors are encouraged to upload their papers in archive.
                </p>

                <h3>Submit</h3>                
                <p class="lead">
                Please use <a href="https://openreview.net/group?id=cv-foundation.org/CVPR/2017/BNMW">OpenReview</a> to submit your paper.

                </p>

                <h3>Proceedings</h3>                
                <p class="lead">
                Will appear soon.
                </p>


                
            </div>
        </div>
    </div>
</section>



<section id="registration">
    <div class="container">
        <div class="row-fluid">
            <div class="span9">
                <h1>Registration & venue</h1>
                <p class="lead">The workshop is together with the <a href="http://cvpr2017.thecvf.com/">Computer Vision and Pattern Recognition (CVPR)</a> 2017.</p>
                <p class="lead">
                 Accepted papers must have at least one registered author (this can be a student). 
                 </p>


                <p class="lead">
                Venue TBD.</p>
                
            </div>
        </div>
    </div>
</section>



<section id="services">
    <div class="container" id="organizers">
        <div class="row-fluid">
            <div class="span9">
                <h1>Organizers</h1>
                <p class="lead">
                <a href="http://www.egavves.com/"> Stratis Gavves</a>,
                <a href="http://users.cecs.anu.edu.au/~basura/">Basura Fernando</a>,
                <a href="https://www.cs.rochester.edu/u/cxu22/">  Chenliang Xu</a>,
                <a href="https://sites.google.com/site/tomyyan555/">  Yan Yan</a>,
                <a href="http://www.robots.ox.ac.uk/~hbilen/">  Hakan Bilen</a>,
                <a href="https://xmhe.bitbucket.io/">  Xuming He</a>,
                <a href="https://sites.google.com/site/michaelyingyang/"> Michael Ying Yang</a>,
                <a href="http://jvgemert.github.io/"> Jan van Gemert.</a> 
                </p>

                
            </div>
        </div>
    </div>
</section>



<!--Footer-->
<footer id="footer">
    <div class="container">
        <div class="row-fluid">
            <div class="span5 cp">
                &copy; 2013 <a target="_blank" href="http://shapebootstrap.net/" title="Free Twitter Bootstrap WordPress Themes and HTML templates">ShapeBootstrap</a>. All Rights Reserved.
            </div>
            <!--/Copyright-->

            <div class="span1">
                <a id="gototop" class="gototop pull-right" href="#"><i class="icon-angle-up"></i></a>
            </div>
            <!--/Goto Top-->
        </div>
    </div>
</footer>
<!--/Footer-->

<script src="js/vendor/jquery-1.9.1.min.js"></script>
<script src="js/vendor/bootstrap.min.js"></script>
<script src="js/main.js"></script>
<!-- Required javascript files for Slider -->
<script src="js/jquery.ba-cond.min.js"></script>
<script src="js/jquery.slitslider.js"></script>
<!-- /Required javascript files for Slider -->

<!-- SL Slider -->
<script type="text/javascript"> 
$(function() {
    var Page = (function() {

        var $navArrows = $( '#nav-arrows' ),
        slitslider = $( '#slider' ).slitslider( {
            autoplay : false
        } ),

        init = function() {
            initEvents();
        },
        initEvents = function() {
            $navArrows.children( ':last' ).on( 'click', function() {
                slitslider.next();
                return false;
            });

            $navArrows.children( ':first' ).on( 'click', function() {
                slitslider.previous();
                return false;
            });
        };

        return { init : init };

    })();

    Page.init();
});
</script>
<!-- /SL Slider -->
</body>
</html>
